{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MortezaMahdaviMortazavi/DeepLearning-Introduction/blob/master/Seq2Seq/FarsiPoemGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXDjzUMglf0e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import Optional , Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxw-b9Gim5Dd"
      },
      "source": [
        "3.2 Attention\n",
        "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
        "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
        "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
        "query with the corresponding key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnj6Ze0Tlqy0"
      },
      "outputs": [],
      "source": [
        "class ScaledAttention(nn.Module):\n",
        "  def __init__(self,sqrt_dim):\n",
        "    super(Attention,self).__init__()\n",
        "    self.sqrt_dim = sqrt_dim\n",
        "\n",
        "  def forward(self,query:Tensor,key:Tensor,value:Tensor,mask:Optional[Tensor]=None) -> Tuple[Tensor,Tensor]:\n",
        "    score = torch.bmm(query,key.transpose(1,2))\n",
        "    scaled_score = score / self.sqrt_dim # scale stage is remove influence of dimension dk\n",
        "    if mask is not None:\n",
        "      scaled_score = torch.mask_fill(mask.view(scaled_score.size()),-float('Inf'))\n",
        "    \n",
        "    attn = F.softmax(scaled_score,-1)\n",
        "    context = torch.bmm(attn,value)\n",
        "    return context , attn # Attention(Q,K,V) = Softmax((QK.T)/(dk)**0.5)V\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0y0dcDGq1BQP"
      },
      "outputs": [],
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "  def __init__(self,hidden_size):\n",
        "    self.query_proj = nn.Linear(hidden_size,hidden_size,bias=False)\n",
        "    self.key_proj = nn.Linear(hidden_size,hidden_size,bias=False)\n",
        "    self.bias = nn.Parameter(torch.randn(hidden_size).uniform_(-0.1,0.1))\n",
        "    self.score_proj = nn.Linear(hidden_size,1)\n",
        "  \n",
        "  def forward(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54xPdf5zrrso"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0Frd_kwy98E"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the flow in more details, let us consider the French\n",
        "translation example where the keys and values are the encoder states\n",
        "for the tokens {J\n",
        "0aime, le, the, . ´ } and the decoder at time t − 1 has\n",
        "generated the tokens I, love. As shown in Fig. 2.6, the decoder output\n",
        "at time t − 1, love, flows into attention layer as a query, which combines\n",
        "with the keys and generates the unnormalized scores a = α(q, k), which\n",
        "then get normalized to give the attention weights b. These attention\n",
        "weights further combine with the values (encoder outputs) to give the\n",
        "context variable or the output vector o. The output then combines with\n",
        "the previous decoder state to generate the next token tea at time t"
      ],
      "metadata": {
        "id": "SFng1eRR3y-S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Lr9K5HSxy_7h"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self,encoder,decoder):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forawrd(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Ci5qTi_3BDM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVasP43O65AgNRVMc+mF9P",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}