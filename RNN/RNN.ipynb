{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME1s2Qa8Ns05cdQNpeMCkm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MortezaMahdaviMortazavi/DeepLearning-Introduction/blob/master/RNN/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCJRZA9eqbYd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getRandomChunk(text,chunk_len=500):\n",
        "  start_idx = np.random.randint(0,len(text)-chunk_len) # because if we set 0 to len(text) it may random chunk length become less than chunk_len\n",
        "  end_idx = start_idx + chunk_len + 1\n",
        "  theChunk = text[start_idx:end_idx]\n",
        "  return theChunk\n",
        "\n",
        "# getRandomChunk(corpus)"
      ],
      "metadata": {
        "id": "uRf2exS-SloF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def char_to_tensor(string):\n",
        "  tensor = torch.zeros((len(string))).long()\n",
        "  for c in range(len(string)):\n",
        "    tensor[c] = all_characters.index(string[c])\n",
        "  return Variable(tensor)"
      ],
      "metadata": {
        "id": "wwkAPA6yUJGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for word to vector you shoud get some sentences and isolate words in count of n and create tensor "
      ],
      "metadata": {
        "id": "diWqx4UGcgTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getChunkOfSentences(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "rOOrOuK2B0RX",
        "outputId": "55d4f18d-8cf6-48be-abb3-235ccc1334b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'And I a tyrant have no leisure taken To weigh how once I suffered in your crime O that our night of woe might have remembered My deepest sense how hard true sorrow hits And soon to you as you to me then tendered The humble salve which wounded bosoms fits But that your trespass now becomes a fee Mine ransoms yours and yours must ransom me Tis better to be vile than vile esteemed When not to be receives reproach of being And the just pleasure lost which is so deemed Not by our feeling but by others seeing For why should others false adulterate eyes Give salutation to my sportive blood Or on my frailties why are frailer spies Which in their wills count bad what I think good '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "class Language:\n",
        "  def __init__(self,name):\n",
        "    self.name = name # name of target language\n",
        "    self.dataset = corpus.split()\n",
        "    self.vocab = len(sorted(corpus)) # all of unique character\n",
        "    self.word2count = {}\n",
        "    self.word2idx = {\n",
        "        \"SOS_TOKEN\":SOS_TOKEN,\n",
        "        \"EOS_TOKEN\":EOS_TOKEN\n",
        "    }\n",
        "    self.idx2word = {\n",
        "        SOS_TOKEN:\"SOS_TOKEN\",\n",
        "        EOS_TOKEN:\"EOS_TOKEN\"\n",
        "    }\n",
        "\n",
        "  def tokenizer(self):\n",
        "    current_idx = 2\n",
        "    for word in self.dataset:\n",
        "      if word not in self.word2idx:\n",
        "        # words = word.replace(string.punctuation,'')\n",
        "        self.word2idx[word] = current_idx\n",
        "        self.idx2word[current_idx] = word\n",
        "        self.word2count[word] = 1\n",
        "        current_idx += 1\n",
        "      else:\n",
        "        self.word2count[word] += 1\n",
        "\n",
        "  @property\n",
        "  def getWord2Idx(self):\n",
        "    return self.word2idx\n",
        "  @property\n",
        "  def getIdx2Word(self):\n",
        "    return self.idx2word\n",
        "\n",
        "  @property\n",
        "  def getWord2Count(self):\n",
        "    return self.word2count\n",
        "\n",
        "  @property\n",
        "  def getCharacters(self):\n",
        "    return self.vocab\n",
        "\n",
        "language = Language('English')"
      ],
      "metadata": {
        "id": "JLXMA7YhuiUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size,num_layers):\n",
        "    super(RNN,self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    self.encoder = nn.Embedding(input_size,hidden_size)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.lstm = nn.LSTM(hidden_size,hidden_size,num_layers,batch_first=True)\n",
        "    self.decoder = nn.Linear(hidden_size,output_size)\n",
        "\n",
        "  def forward(self,X,hidden,cell):\n",
        "    X = X.to(self.device)\n",
        "    output = self.encoder(X)\n",
        "    output = self.dropout(output)\n",
        "    output , (hidden,cell) = self.lstm(output.unsqueeze(1),(hidden,cell))\n",
        "    output = self.decoder(output.reshape(output.shape[0],-1))\n",
        "    # output = self.decoder(output)\n",
        "    return output , hidden , cell\n",
        "    \n",
        "\n",
        "  def init_hidden(self,batch_size):\n",
        "    hidden = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(self.device)\n",
        "    cell = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(self.device)\n",
        "\n",
        "    hidden = Variable(hidden)\n",
        "    cell = Variable(cell)\n",
        "    return hidden , cell\n"
      ],
      "metadata": {
        "id": "miXhMmmo0M8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_testing():\n",
        "  X = word2vec(lang=language,sentences=sentences)\n",
        "  model = RNN(len(X),len(X),len(X),num_layers=25)\n",
        "  hidden , cell = model.init_hidden(1)\n",
        "  out = model.forward(X,hidden,cell)\n",
        "  print(f\"output shape is {out.shape}\")\n",
        "  print(f\"hidden and cell shape is {hidden.shape}\")\n",
        "  print(f\"out is {out}\")\n",
        "# X = word2vec(lang=language,sentences=sentences)\n",
        "# model = RNN(len(word2idx),len(X),len(X),num_layers=25)\n",
        "# hidden , cell = model.init_hidden(100)\n",
        "# model.forward(X,hidden,cell)[0]"
      ],
      "metadata": {
        "id": "d9xgXFCH3J_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator:\n",
        "  def __init__(self,file):\n",
        "    super().__init__()\n",
        "    self.n_sentences = 15\n",
        "    self.max_words = 100\n",
        "    self.batch_size = 15\n",
        "    self.hidden_size = 256\n",
        "    self.num_layers = 10\n",
        "    self.lr = 0.005\n",
        "    self.sentences = self.corpusProcessing(file)[0]\n",
        "    self.corpus = self.corpusProcessing(file)[1]\n",
        "    self.language = None  \n",
        "    self.rnn = None\n",
        "    self.optimizer = None\n",
        "    self.criterion = None\n",
        "\n",
        "  def setModel(self,input_size,hidden_size,num_layers):\n",
        "    self.rnn = RNN(\n",
        "        input_size=input_size,\n",
        "        hidden_size=hidden_size,\n",
        "        output_size = hidden_size,\n",
        "        num_layers = num_layers\n",
        "      )\n",
        "    \n",
        "  def setLanguage(self,name):\n",
        "    self.language = Language(name)\n",
        "  \n",
        "  def setOptimizer(self,model):\n",
        "    self.optimizer = torch.optim.Adam(filter(\n",
        "        lambda p:p.requires_grad(),model.parameters()),lr=self.lr)\n",
        "    \n",
        "  def setOptimizer(self):\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  def corpusProcessing(self,file):\n",
        "    corpus = open(file,'r').read() # read text file\n",
        "    corpus.translate(str.maketrans('','',string.punctuation)) # try to eliminate punctuations\n",
        "    new_corpus = re.sub(r'[^\\w\\s]','',corpus) # eliminate all punctuations that remind\n",
        "    all_characters = string.printable # all characters that exists in keyboard\n",
        "    n_characters = len(all_characters) # length of characters\n",
        "    sentences = new_corpus.split('\\n') # split corpus base on sentences\n",
        "    for item in sentences:\n",
        "      if item == '' or item == ' ' or item == '  ' or item == '   ': # delete all sentences that are empty\n",
        "        sentences.pop(sentences.index(item))\n",
        "    return sentences , new_corpus\n",
        "\n",
        "\n",
        "\n",
        "  def getChunkOfSentences(self,sentences=self.sentences,n_sentence=self.n_sentences):\n",
        "    start_idx = np.random.randint(0,len(sentences)-n_sentence)\n",
        "    end_idx = start_idx + n_sentence + 1\n",
        "    chunk_sentences = sentences[start_idx:end_idx]\n",
        "    theChunk = ''\n",
        "    for sen in chunk_sentences:\n",
        "      theChunk += (sen) + ' '\n",
        "    return theChunk\n",
        "\n",
        "  def word2vec(self,lang,chunk_corpus,sentences=self.sentences,max_length=self.max_words):\n",
        "    # Default argument values are evaluated at function define-time, but self is an argument only available at function call time.\n",
        "    # Thus arguments in the argument list cannot refer each other\n",
        "    # chunk_corpus = getChunkOfSentences(sentences=sentences,n_sentence=15) # get a chunk of corpus\n",
        "    separate_words = chunk_corpus.split()[:max_length] # separate words in number of max_length\n",
        "    tensor = torch.zeros(max_length).long() # create a tensor of zeros in length of max_length\n",
        "    word2idx = lang.getWord2Idx \n",
        "    for idx in range(max_length): \n",
        "      tensor[idx] = word2idx[separate_words[idx]] # for each index of tensor,add word2idx relation index\n",
        "    \n",
        "    return tensor\n",
        "\n",
        "  def get_random_batch(self):\n",
        "    chunks = []\n",
        "    for i in range(self.batch_size):\n",
        "      chunk = self.getChunkOfSentences()\n",
        "      chunks.append(chunk)\n",
        "\n",
        "    tensor_input = torch.zeros(self.batch_size,self.max_words)\n",
        "    tensor_target = torch.zeros(self.batch_size,self.max_words)\n",
        "\n",
        "    for i in range(self.batch_size):\n",
        "      tensor_input[i:] = self.word2vec(lang=self.language,chunk_corpus=chunks[i])[:-1]\n",
        "      tensor_target[i:] = self.word2vec(lang=self.language,chunk_corpus=chunks[i])[1:]\n",
        "\n",
        "    tensor_input = tensor_input.long()\n",
        "    tensor_target = tensor_target.long()\n",
        "\n",
        "    return tensor_input , tensor_target\n",
        "\n",
        "  def train(self):\n",
        "    pass\n",
        "\n",
        "  def generate(self):\n",
        "    pass  "
      ],
      "metadata": {
        "id": "0oHTM5mlGEE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "280ffbdd-fa83-473f-fa81-88978590a120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-396a8a2eec09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-396a8a2eec09>\u001b[0m in \u001b[0;36mGenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mgetChunkOfSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mstart_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mend_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_sentence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences , corpus = corpusProcessing('shakespeare.txt')\n",
        "corpus.split()[:10]"
      ],
      "metadata": {
        "id": "I2_QYtYaGFnR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}