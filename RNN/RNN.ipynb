{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBaSX3yPsi5WfWUKPUW7cY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MortezaMahdaviMortazavi/DeepLearning-Introduction/blob/master/RNN/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zCJRZA9eqbYd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getRandomChunk(text,chunk_len=500):\n",
        "  start_idx = np.random.randint(0,len(text)-chunk_len) # because if we set 0 to len(text) it may random chunk length become less than chunk_len\n",
        "  end_idx = start_idx + chunk_len + 1\n",
        "  theChunk = text[start_idx:end_idx]\n",
        "  return theChunk\n",
        "\n",
        "# getRandomChunk(corpus)"
      ],
      "metadata": {
        "id": "uRf2exS-SloF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def char_to_tensor(string):\n",
        "  tensor = torch.zeros((len(string))).long()\n",
        "  for c in range(len(string)):\n",
        "    tensor[c] = all_characters.index(string[c])\n",
        "  return Variable(tensor)"
      ],
      "metadata": {
        "id": "wwkAPA6yUJGc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for word to vector you shoud get some sentences and isolate words in count of n and create tensor "
      ],
      "metadata": {
        "id": "diWqx4UGcgTz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "class Language:\n",
        "  def __init__(self,corpus,name):\n",
        "    self.name = name # name of target language\n",
        "    self.dataset = corpus.split()\n",
        "    self.vocab = len(sorted(corpus)) # all of unique character\n",
        "    self.word2count = {}\n",
        "    self.word2idx = {\n",
        "        \"SOS_TOKEN\":SOS_TOKEN,\n",
        "        \"EOS_TOKEN\":EOS_TOKEN\n",
        "    }\n",
        "    self.idx2word = {\n",
        "        SOS_TOKEN:\"SOS_TOKEN\",\n",
        "        EOS_TOKEN:\"EOS_TOKEN\"\n",
        "    }\n",
        "\n",
        "  def tokenizer(self):\n",
        "    current_idx = 2\n",
        "    for word in self.dataset:\n",
        "      if word not in self.word2idx:\n",
        "        # words = word.replace(string.punctuation,'')\n",
        "        self.word2idx[word] = current_idx\n",
        "        self.idx2word[current_idx] = word\n",
        "        self.word2count[word] = 1\n",
        "        current_idx += 1\n",
        "      else:\n",
        "        self.word2count[word] += 1\n",
        "\n",
        "  @property\n",
        "  def getWord2Idx(self):\n",
        "    return self.word2idx\n",
        "  @property\n",
        "  def getIdx2Word(self):\n",
        "    return self.idx2word\n",
        "\n",
        "  @property\n",
        "  def getWord2Count(self):\n",
        "    return self.word2count\n",
        "\n",
        "  @property\n",
        "  def getCharacters(self):\n",
        "    return self.vocab\n",
        "\n",
        "corpus = open('shakespeare.txt','r').read()\n",
        "language = Language(corpus=corpus,name='English')"
      ],
      "metadata": {
        "id": "JLXMA7YhuiUo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size,num_layers):\n",
        "    super(RNN,self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    self.encoder = nn.Embedding(input_size,hidden_size)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.lstm = nn.LSTM(hidden_size,hidden_size,num_layers,batch_first=True)\n",
        "    self.decoder = nn.Linear(hidden_size,output_size)\n",
        "\n",
        "  def forward(self,X,hidden,cell):\n",
        "    X = X.to(self.device)\n",
        "    output = self.encoder(X)\n",
        "    output = self.dropout(output)\n",
        "    output , (hidden,cell) = self.lstm(output.unsqueeze(1),(hidden,cell))\n",
        "    output = self.decoder(output.reshape(output.shape[0],-1))\n",
        "    # output = self.decoder(output)\n",
        "    return output , hidden , cell\n",
        "    \n",
        "\n",
        "  def init_hidden(self,batch_size):\n",
        "    hidden = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(self.device)\n",
        "    cell = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(self.device)\n",
        "\n",
        "    hidden = Variable(hidden)\n",
        "    cell = Variable(cell)\n",
        "    return hidden , cell\n"
      ],
      "metadata": {
        "id": "miXhMmmo0M8i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_testing():\n",
        "  X = word2vec(lang=language,sentences=sentences)\n",
        "  model = RNN(len(X),len(X),len(X),num_layers=25)\n",
        "  hidden , cell = model.init_hidden(1)\n",
        "  out = model.forward(X,hidden,cell)\n",
        "  print(f\"output shape is {out.shape}\")\n",
        "  print(f\"hidden and cell shape is {hidden.shape}\")\n",
        "  print(f\"out is {out}\")\n",
        "# X = word2vec(lang=language,sentences=sentences)\n",
        "# model = RNN(len(word2idx),len(X),len(X),num_layers=25)\n",
        "# hidden , cell = model.init_hidden(100)\n",
        "# model.forward(X,hidden,cell)[0]"
      ],
      "metadata": {
        "id": "d9xgXFCH3J_W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator:\n",
        "  def __init__(self,file):\n",
        "    super().__init__()\n",
        "    self.n_sentences = 15\n",
        "    self.max_words = 100\n",
        "    self.batch_size = 1\n",
        "    self.hidden_size = 256\n",
        "    self.num_layers = 10\n",
        "    self.lr = 0.005\n",
        "    self.epochs = 10\n",
        "    self.sentences = self.corpusProcessing(file)[0]\n",
        "    self.corpus = self.corpusProcessing(file)[1]\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.language = None  \n",
        "    self.rnn = None\n",
        "    self.optimizer = None\n",
        "    self.criterion = None\n",
        "    \n",
        "\n",
        "  def setModel(self,input_size,hidden_size,num_layers):\n",
        "    self.rnn = RNN(\n",
        "        input_size=input_size,\n",
        "        hidden_size=hidden_size,\n",
        "        output_size = hidden_size,\n",
        "        num_layers = num_layers\n",
        "      )\n",
        "  \n",
        "  def tokenize(self):\n",
        "    self.language.tokenizer()\n",
        "    \n",
        "  def setLanguage(self,name):\n",
        "    self.language = Language(self.corpus,name)\n",
        "    self.tokenize()\n",
        "  \n",
        "  def setOptimizer(self):\n",
        "    self.optimizer = torch.optim.Adam(filter(\n",
        "        lambda p:p.requires_grad,self.rnn.parameters()),lr=self.lr)\n",
        "    \n",
        "  def setCriterion(self):\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "  def corpusProcessing(self,file):\n",
        "    corpus = open(file,'r').read() # read text file\n",
        "    corpus.translate(str.maketrans('','',string.punctuation)) # try to eliminate punctuations\n",
        "    new_corpus = re.sub(r'[^\\w\\s]','',corpus) # eliminate all punctuations that remind\n",
        "    all_characters = string.printable # all characters that exists in keyboard\n",
        "    n_characters = len(all_characters) # length of characters\n",
        "    sentences = new_corpus.split('\\n') # split corpus base on sentences\n",
        "    for item in sentences:\n",
        "      if item == '' or item == ' ' or item == '  ' or item == '   ': # delete all sentences that are empty\n",
        "        sentences.pop(sentences.index(item))\n",
        "    return sentences , new_corpus\n",
        "\n",
        "\n",
        "\n",
        "  def getChunkOfSentences(self):\n",
        "    start_idx = np.random.randint(0,len(self.sentences)-self.n_sentences)\n",
        "    end_idx = start_idx + self.n_sentences + 1\n",
        "    chunk_sentences = self.sentences[start_idx:end_idx]\n",
        "    theChunk = ''\n",
        "    for sen in chunk_sentences:\n",
        "      theChunk += (sen) + ' '\n",
        "    return theChunk\n",
        "\n",
        "  def word2vec(self,lang,chunk_corpus):\n",
        "    # Default argument values are evaluated at function define-time, but self is an argument only available at function call time.\n",
        "    # Thus arguments in the argument list cannot refer each other\n",
        "    # chunk_corpus = getChunkOfSentences(sentences=sentences,n_sentence=15) # get a chunk of corpus\n",
        "    separate_words = chunk_corpus.split()[:self.max_words] # separate words in number of max_length\n",
        "    # print(len(separate_words))\n",
        "    tensor = torch.zeros(self.max_words).long() # create a tensor of zeros in length of max_length\n",
        "    word2idx = lang.getWord2Idx\n",
        "    for idx in range(self.max_words): \n",
        "      tensor[idx] = word2idx[separate_words[idx]] # for each index of tensor,add word2idx relation index\n",
        "    \n",
        "    return tensor\n",
        "\n",
        "  def get_random_batch(self):\n",
        "    chunks = []\n",
        "    for i in range(self.batch_size):\n",
        "      chunk = self.getChunkOfSentences()\n",
        "      chunk = chunk.strip()\n",
        "      chunks.append(chunk)\n",
        "    # return chunks\n",
        "    tensor_input = torch.zeros(self.batch_size,self.max_words-1)\n",
        "    tensor_target = torch.zeros(self.batch_size,self.max_words-1)\n",
        "\n",
        "    for i in range(self.batch_size):\n",
        "      # w2vec = self.word2vec(lang=self.language,chunk_corpus=chunks[i])\n",
        "      # tensor_input[i,:] = w2vec[:-1]\n",
        "      # tensor_target[i,:] = w2vec[1:]\n",
        "      tensor_input[i,:] = self.word2vec(lang=self.language,chunk_corpus=chunks[i])[:-1]\n",
        "      tensor_target[i,:] = self.word2vec(lang=self.language,chunk_corpus=chunks[i])[1:]\n",
        "      # break\n",
        "      \n",
        "    tensor_input = tensor_input.long()\n",
        "    tensor_target = tensor_target.long()\n",
        "\n",
        "    return tensor_input , tensor_target\n",
        "\n",
        "  def train(self):\n",
        "    self.rnn = self.rnn.to(self.device)\n",
        "    print(\"************ Training is start *********\")\n",
        "    for epoch in range(self.epochs+1):\n",
        "      try:\n",
        "        inp , target = self.get_random_batch()\n",
        "        hidden , cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
        "\n",
        "        self.rnn.zero_grad()\n",
        "        loss = 0\n",
        "        inp = inp.to(self.device)\n",
        "        target = target.to(self.device)\n",
        "\n",
        "        for c in range(self.max_words-1):\n",
        "          # return self.rnn(inp[:,c],hidden,cell)\n",
        "          # break\n",
        "          # print(inp[:,c] , target[:,c])\n",
        "          out , hidden,cell = self.rnn(inp[:,c],hidden,cell)\n",
        "          loss += self.criterion(out,target[:,c])\n",
        "\n",
        "\n",
        "        self.loss.backward()\n",
        "        self.optimizer.step()\n",
        "        loss = self.loss.item()/len(self.batch_size)\n",
        "        print(f\"loss is {loss}\")\n",
        "        \n",
        "        if epoch % 100 == 0:\n",
        "          print(f\"In epoch {epoch} loss is: {loss}\")\n",
        "      except:\n",
        "        print(f\"ExceptionError raised in epoch {epoch}\")\n",
        "      \n",
        "\n",
        "  def generate(self):\n",
        "    pass"
      ],
      "metadata": {
        "id": "0oHTM5mlGEE1"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator('shakespeare.txt')\n",
        "generator.setLanguage('English')\n",
        "input_size = len(generator.language.getWord2Idx)\n",
        "hidden_size = 256\n",
        "num_layers = 25\n",
        "# rnn = RNN(input_size=input_size,hidden_size=hidden_size,output_size=output_size,num_layers=num_layers)\n",
        "generator.setModel(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers)\n",
        "generator.setOptimizer()\n",
        "generator.setCriterion()\n",
        "generator.language.getWord2Idx\n",
        "generator.train()"
      ],
      "metadata": {
        "id": "I2_QYtYaGFnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb04a203-86bb-45fd-c2a3-505acbf9cc8c"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************ Training is start *********\n",
            "ExceptionError raised in epoch 0\n",
            "ExceptionError raised in epoch 1\n",
            "ExceptionError raised in epoch 2\n",
            "ExceptionError raised in epoch 3\n",
            "ExceptionError raised in epoch 4\n",
            "ExceptionError raised in epoch 5\n",
            "ExceptionError raised in epoch 6\n",
            "ExceptionError raised in epoch 7\n",
            "ExceptionError raised in epoch 8\n",
            "ExceptionError raised in epoch 9\n",
            "ExceptionError raised in epoch 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "medrya1q-tHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}