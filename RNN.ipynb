{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoPBQ/3q73HOmDJ1kK+AqL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MortezaMahdaviMortazavi/DeepLearning-Introduction/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zCJRZA9eqbYd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def corpusProcessing(file):\n",
        "  corpus = open(file,'r').read() # read text file\n",
        "  corpus.translate(str.maketrans('','',string.punctuation)) # try to eliminate punctuations\n",
        "  new_corpus = re.sub(r'[^\\w\\s]','',corpus) # eliminate all punctuations that remind\n",
        "  all_characters = string.printable # all characters that exists in keyboard\n",
        "  n_characters = len(all_characters) # length of characters\n",
        "  sentences = new_corpus.split('\\n') # split corpus base on sentences\n",
        "  for item in sentences:\n",
        "    if item == '' or item == ' ' or item == '  ' or item == '   ': # delete all sentences that are empty\n",
        "      sentences.pop(sentences.index(item))\n",
        "  return sentences , new_corpus\n",
        "\n",
        "sentences , corpus = corpusProcessing('shakespeare.txt')\n",
        "corpus.split()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaEjeXfG-lzU",
        "outputId": "8ad97106-633d-45da-95d0-ba66ce822024"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['THE',\n",
              " 'SONNETS',\n",
              " 'by',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " 'From',\n",
              " 'fairest',\n",
              " 'creatures',\n",
              " 'we',\n",
              " 'desire']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getRandomChunk(text,chunk_len=500):\n",
        "  start_idx = np.random.randint(0,len(text)-chunk_len) # because if we set 0 to len(text) it may random chunk length become less than chunk_len\n",
        "  end_idx = start_idx + chunk_len + 1\n",
        "  theChunk = text[start_idx:end_idx]\n",
        "  return theChunk\n",
        "\n",
        "# getRandomChunk(corpus)"
      ],
      "metadata": {
        "id": "uRf2exS-SloF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6628cab5-7402-4ab7-9aea-51c0aadacc3c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'juicehealth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def char_to_tensor(string):\n",
        "  tensor = torch.zeros((len(string))).long()\n",
        "  for c in range(len(string)):\n",
        "    tensor[c] = all_characters.index(string[c])\n",
        "  return Variable(tensor)"
      ],
      "metadata": {
        "id": "wwkAPA6yUJGc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for word to vector you shoud get some sentences and isolate words in count of n and create tensor "
      ],
      "metadata": {
        "id": "diWqx4UGcgTz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getChunkOfSentences(sentences,n_sentence=15):\n",
        "  start_idx = np.random.randint(0,len(sentences)-n_sentence)\n",
        "  end_idx = start_idx + n_sentence + 1\n",
        "  chunk_sentences = sentences[start_idx:end_idx]\n",
        "  theChunk = ''\n",
        "  for sen in chunk_sentences:\n",
        "    theChunk += (sen) + ' '\n",
        "  return theChunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-B-fDsp6wDE",
        "outputId": "af1db7a7-6be7-4140-ff12-838b4ed48b6d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getChunkOfSentences(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "rOOrOuK2B0RX",
        "outputId": "27d96c5d-d495-45d4-e201-e586acc36751"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Kill me with spites yet we must not be foes Those pretty wrongs that liberty commits When I am sometime absent from thy heart Thy beauty and thy years full well befits For still temptation follows where thou art Gentle thou art and therefore to be won Beauteous thou art therefore to be assailed And when a woman woos what womans son Will sourly leave her till he have prevailed Ay me but yet thou mightst my seat forbear And chide thy beauty and thy straying youth Who lead thee in their riot even there Where thou art forced to break a twofold truth Hers by thy beauty tempting her to thee Thine by thy beauty being false to me That thou hast her it is not all my grief '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "class Language:\n",
        "  def __init__(self,name):\n",
        "    self.name = name # name of target language\n",
        "    self.dataset = corpus.split()\n",
        "    self.vocab = len(sorted(corpus)) # all of unique character\n",
        "    self.word2count = {}\n",
        "    self.word2idx = {\n",
        "        \"SOS_TOKEN\":SOS_TOKEN,\n",
        "        \"EOS_TOKEN\":EOS_TOKEN\n",
        "    }\n",
        "    self.idx2word = {\n",
        "        SOS_TOKEN:\"SOS_TOKEN\",\n",
        "        EOS_TOKEN:\"EOS_TOKEN\"\n",
        "    }\n",
        "\n",
        "  def tokenizer(self):\n",
        "    current_idx = 2\n",
        "    for word in self.dataset:\n",
        "      if word not in self.word2idx:\n",
        "        # words = word.replace(string.punctuation,'')\n",
        "        self.word2idx[word] = current_idx\n",
        "        self.idx2word[current_idx] = word\n",
        "        self.word2count[word] = 1\n",
        "        current_idx += 1\n",
        "      else:\n",
        "        self.word2count[word] += 1\n",
        "\n",
        "  @property\n",
        "  def getWord2Idx(self):\n",
        "    return self.word2idx\n",
        "  @property\n",
        "  def getIdx2Word(self):\n",
        "    return self.idx2word\n",
        "\n",
        "  @property\n",
        "  def getWord2Count(self):\n",
        "    return self.word2count\n",
        "\n",
        "  @property\n",
        "  def getCharacters(self):\n",
        "    return self.vocab\n",
        "\n",
        "language = Language('English')"
      ],
      "metadata": {
        "id": "JLXMA7YhuiUo"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2vec(lang,sentences,max_length=100):\n",
        "  # get a chunk of corpus\n",
        "  # separate words in number of max_length\n",
        "  # create a tensor of zeros in length of max_length\n",
        "  # for each index of tensor,add word2idx relation index\n",
        "  chunk_corpus = getChunkOfSentences(sentences=sentences,n_sentence=15)\n",
        "  separate_words = chunk_corpus.split()[:max_length]\n",
        "  tensor = torch.zeros(max_length)\n",
        "  word2idx = lang.getWord2Idx\n",
        "  for idx in range(max_length):\n",
        "    tensor[idx] = word2idx[separate_words[idx]]\n",
        "  \n",
        "  return tensor"
      ],
      "metadata": {
        "id": "0qtLL2Q8WiiG"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language = Language('English')\n",
        "language.tokenizer()\n",
        "word2idx = language.getWord2Idx\n",
        "idx2word = language.getIdx2Word\n",
        "word2count = language.getWord2Count\n",
        "word2vec(language,sentences=sentences)"
      ],
      "metadata": {
        "id": "PBX1iENExNgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7a324b-155b-44df-d1d3-44d5ba869ce0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  20.,  740., 1304.,  194.,  325.,  233.,  359., 1305.,  145.,  244.,\n",
              "        1306., 1307.,  507., 1308.,  299.,  406.,  550.,   62.,  401.,  299.,\n",
              "         354.,   84.,  148.,  513.,  322., 1309., 1310., 1311.,  112.,   22.,\n",
              "          81.,  498., 1312.,  158.,  942.,  934., 1313.,  439., 1314.,   33.,\n",
              "        1315.,  519.,   48.,  241.,  414.,   66.,  150.,  550., 1096., 1316.,\n",
              "        1317.,  148., 1318.,   87.,  336., 1319., 1144., 1320.,  550.,   78.,\n",
              "         148.,  421., 1321.,   41., 1322.,   78.,  332., 1323., 1324., 1325.,\n",
              "         177., 1326.,   60.,  210.,   22., 1307.,   33., 1181.,   87., 1327.,\n",
              "          22.,  689.,  108.,  148., 1328.,  168.,  184.,  322.,  907.,  497.,\n",
              "         112.,  519.,   48., 1329.,  630., 1330.,   13., 1331.,   22., 1332.])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size,num_layers):\n",
        "    super(RNN,self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.encoder = nn.Embedding(input_size,hidden_size)\n",
        "    self.lstm = nn.LSTM(hidden_size,hidden_size,num_layers,batch_first=True)\n",
        "    self.decoder = nn.Linear(hidden_size,output_size)\n",
        "\n",
        "  def forward(self,X,hidden,cell):\n",
        "    pass\n",
        "\n",
        "  def init_hidden(self,batch_size):\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "miXhMmmo0M8i"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator:\n",
        "  pass"
      ],
      "metadata": {
        "id": "0oHTM5mlGEE1"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2_QYtYaGFnR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}